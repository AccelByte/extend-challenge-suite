# M2 Performance Optimization - Technical Specification

**Last Updated:** 2025-10-29
**Version:** 3.0 (Challenge Service + Event Handler Optimization Complete)
**Status:** ‚úÖ Production Ready (Both Services Validated)

---

## Executive Summary

### Overall Achievement

This document covers performance optimization for **two separate services**:

1. **Challenge Service (REST API):** 200 RPS ‚Üí 300-350 RPS (1.5-1.75x improvement, CPU-limited)
2. **Event Handler (gRPC Events):** 239 EPS ‚Üí 494 EPS (2.07x improvement)

### Challenge Service Optimization

üéØ **Target:** Increase throughput from 200 RPS to 500+ RPS
‚úÖ **Result:** **~300-350 RPS safe capacity** (1.5-1.75x improvement, CPU-limited)

| Phase | Optimizations | CPU Reduction | Throughput Gain | Status |
|-------|--------------|---------------|-----------------|--------|
| **Phase 1** | Memory & Buffer Optimizations | 44% memory, 16% CPU | Baseline stability | ‚úÖ Complete |
| **Phase 2** | String Injection (JSON) | 56% JSON ops | **1.5-1.75x** | ‚úÖ Complete |
| **Total** | 6 optimizations implemented | ~60% overall | **1.5-1.75x** | ‚úÖ Complete |

**Note:** System hits CPU saturation at 400 RPS (101% CPU, 397x latency degradation). Safe operational capacity is 300-350 RPS.

### Event Handler Optimization

üéØ **Target:** Process 500 EPS with <500ms P95 latency
‚úÖ **Result:** **494 EPS capacity** (98.7% of target, 2.07x improvement)

| Phase | Optimizations | Key Changes | Result | Status |
|-------|--------------|-------------|--------|--------|
| **Phase 1** | Backpressure Investigation | Baseline measurement | 239 EPS, 48% loss | ‚úÖ Analysis |
| **Phase 2** | PostgreSQL COPY Protocol | Temp table bulk insert | 308 EPS, 99.99% success | ‚úÖ Complete |
| **Phase 3** | Flush Interval Tuning | 1000ms‚Üí100ms, 3000 buffer | 308 EPS (still bottlenecked) | ‚úÖ Complete |
| **Phase 3.5** | PostgreSQL 4 CPUs | 2 CPUs ‚Üí 4 CPUs | 475 EPS (high latency) | ‚úÖ Complete |
| **Phase 4** | 8 Parallel Flush Workers | Hash-based partitioning | 474 EPS (low latency) | ‚úÖ Complete |
| **Phase 4b** | k6 Load Generator Fix | 500‚Üí1000/1500 VUs | **494 EPS ‚úÖ** | ‚úÖ Complete |
| **Total** | 5 major optimizations | PostgreSQL + parallelization | **2.07x** | ‚úÖ Complete |

### Key Metrics Comparison

**Challenge Service (REST API):**

| Metric | Baseline | Phase 1 | Phase 2 (Final) | Improvement |
|--------|----------|---------|-----------------|-------------|
| **Throughput (Safe)** | 200 RPS @ 101% CPU | 200 RPS @ 85% CPU | **300-350 RPS @ 65-75% CPU** | **+50-75%** |
| **Throughput (Max)** | 200 RPS @ 101% CPU | 200 RPS @ 85% CPU | **400 RPS @ 101% CPU ‚ö†Ô∏è** | **+100% (saturated)** |
| **Memory Alloc** | 161 GB/min | 88.7 GB/min | ~60 GB/min (est) | **-63%** |
| **JSON CPU** | 13.87s (46%) | 13.87s (46%) | **6.15s (20%)** | **-56%** |
| **p95 Latency** | ~200ms | 4-6ms | **4.04ms @ 300 RPS** | **98% faster** |
| **p95 Latency (Max)** | ~200ms | 4-6ms | **1,442ms @ 400 RPS ‚ö†Ô∏è** | **622% slower** |
| **Stability** | OOM crashes | Stable | **Stable @ 300-350 RPS** | ‚úÖ |

**‚ö†Ô∏è CPU Saturation:** System hits hard limit at 400 RPS (101% CPU, 397x latency degradation). Safe operational capacity is 300-350 RPS.

**Event Handler (gRPC Events):**

| Metric | Phase 1 (Baseline) | Phase 2 (COPY) | Phase 4 (8 Workers) | Phase 4b (Final) | Improvement |
|--------|-------------------|----------------|---------------------|------------------|-------------|
| **Throughput** | 239 EPS (48% loss) | 308 EPS | 474 EPS | **494 EPS** | **+107%** |
| **Success Rate** | 52.0% | 99.99% | 100% | **100%** | **+48pp** |
| **P95 Latency** | 10,000ms | 302ms | 44ms | **21ms** | **99.8% faster** |
| **P99 Latency** | ~10,000ms | 437ms | 98ms | **45ms** | **99.5% faster** |
| **PG CPU** | 100% (bottleneck) | 67% | 25% | **25%** | **-75%** |
| **Backpressure** | 47,990 activations | 0 | 0 | **0** | **100% eliminated** |

---

## Table of Contents

### Part 1: Challenge Service (REST API) Optimization
- [Phase 1: Memory & Buffer Optimizations](#phase-1-memory--buffer-optimizations)
- [Phase 2: String Injection (JSON Optimization)](#phase-2-string-injection-json-optimization)
- [Challenge Service Architecture](#current-system-architecture)
- [Challenge Service Performance](#performance-characteristics)

### Part 2: Event Handler (gRPC Events) Optimization
- [Event Handler Phase 1: Backpressure Investigation](#event-handler-phase-1-backpressure-investigation)
- [Event Handler Phase 2: PostgreSQL COPY Protocol](#event-handler-phase-2-postgresql-copy-protocol)
- [Event Handler Phase 3: Flush Interval Tuning](#event-handler-phase-3-flush-interval-tuning)
- [Event Handler Phase 3.5: PostgreSQL 4 CPUs](#event-handler-phase-35-postgresql-4-cpus)
- [Event Handler Phase 4: 8 Parallel Flush Workers](#event-handler-phase-4-8-parallel-flush-workers)
- [Event Handler Phase 4b: k6 Load Generator Fix](#event-handler-phase-4b-k6-load-generator-fix)
- [Event Handler Architecture](#event-handler-architecture)

### Part 3: Combined Load Testing
- [Phase 5: Combined Load Testing (300 RPS + 500 EPS)](#phase-5-combined-load-testing-300-rps--500-eps)

### Part 4: Deployment & Conclusion
- [Deployment Recommendations](#deployment-recommendations)
- [Future Optimization Opportunities](#future-optimization-opportunities)
- [Conclusion](#conclusion)

---

# Part 1: Challenge Service (REST API) Optimization

---

## Phase 1: Memory & Buffer Optimizations

**Date:** 2025-10-24
**Goal:** Fix OOM crashes and reduce memory pressure
**Result:** ‚úÖ Service stable, 44% memory reduction

### 1.1 ‚úÖ Critical Bug Fix: Sonic Intermediate Unmarshal

**Problem:** Unnecessary unmarshal step in Sonic marshaler causing 32GB/min waste

**Files Changed:**
- `pkg/common/sonic_marshaler.go:52-60` - Removed intermediate unmarshal in `Marshal()`
- `pkg/common/sonic_marshaler.go:114-135` - Removed intermediate unmarshal in `Encode()`

**Before:**
```go
func (sm *SonicMarshaler) Marshal(v interface{}) ([]byte, error) {
    // Bug: Unnecessary unmarshal of protojson output
    var temp map[string]interface{}
    sonic.Unmarshal(protojsonBytes, &temp)  // ‚ùå 32 GB/min waste
    return sonic.Marshal(temp)
}
```

**After:**
```go
func (sm *SonicMarshaler) Marshal(v interface{}) ([]byte, error) {
    // Direct return protojson output (already valid JSON)
    return protojsonBytes, nil  // ‚úÖ Zero allocation
}
```

**Impact:**
- Memory: -31,990 MB/min (-19.86%)
- CPU: ~15-20% reduction in memory operations
- Status: ‚úÖ **Completely eliminated from profile**

---

### 1.2 ‚úÖ gRPC Buffer Tuning

**Problem:** gRPC creating too many small buffers (17.8 GB/min)

**Files Changed:**
- `pkg/common/gateway.go:46-54` - Configured buffer sizes

**Configuration:**
```go
grpc.WriteBufferSize(32 * 1024),      // 32KB write buffer
grpc.ReadBufferSize(32 * 1024),       // 32KB read buffer
grpc.InitialWindowSize(64 * 1024),    // 64KB initial window
grpc.InitialConnWindowSize(128 * 1024), // 128KB connection window
```

**Impact:**
- Memory: -15,930 MB/min (-89.4% reduction in gRPC buffers)
- CPU: ~2-3% reduction
- Status: ‚úÖ **Exceeded expectations**

---

### 1.3 ‚úÖ Time Formatting Optimization

**Problem:** String allocations for RFC3339 time formatting

**Files Changed:**
- `pkg/mapper/challenge_mapper.go:75-84` - Use `AppendFormat` with buffer reuse
- `pkg/mapper/challenge_mapper.go:105-118` - Direct date comparison (no string allocation)

**Before:**
```go
goal.CompletedAt = progress.CompletedAt.Format(time.RFC3339)  // ‚ùå Allocation per call
```

**After:**
```go
buf := make([]byte, 0, 32)
buf = progress.CompletedAt.AppendFormat(buf, time.RFC3339)  // ‚úÖ Reusable buffer
goal.CompletedAt = string(buf)
```

**Impact:**
- Memory: ~3,000 MB/min reduction
- CPU: ~2-3% reduction
- Status: ‚úÖ **Not visible in top allocators (<1%)**

---

### 1.4 ‚ùå Object Pooling (FAILED - Race Condition)

**Problem:** Protobuf object allocations (10 GB/min)

**Attempted Solution:**
```go
var challengePool = sync.Pool{
    New: func() interface{} { return &pb.Challenge{} },
}

// In handler:
challenge := challengePool.Get().(*pb.Challenge)
defer challengePool.Put(challenge)  // ‚ùå RACE CONDITION
```

**Why It Failed:**
- **gRPC serializes AFTER handler returns** (async)
- Objects returned to pool while still being read by gRPC serializer
- Result: Memory corruption, data races, OOM crashes

**Current Status:**
- Pool creation code remains (creates objects from pool)
- Returns REMOVED (objects GC'd normally)
- Net result: **No benefit, but no harm**

**Lesson Learned:** Cannot pool objects that are async-serialized by gRPC

---

## Phase 2: String Injection (JSON Optimization)

**Date:** 2025-10-25
**Goal:** Eliminate protojson marshaling bottleneck (46% CPU)
**Result:** ‚úÖ **100% elimination**, 1.5-2x throughput gain (300-400 RPS, CPU-limited)

### 2.1 Architecture: Zero-Copy String Injection

**Problem:** Protojson marshaling entire challenge responses consuming 46% CPU

**Original Approach (Slow):**
```
Static Challenge Config ‚Üí protojson.Marshal ‚Üí JSON bytes
                                ‚Üì
User Progress Data ‚Üí protojson.Marshal ‚Üí Progress JSON
                                ‚Üì
                    json.Unmarshal both ‚Üí map[string]interface{}
                                ‚Üì
                        Merge maps
                                ‚Üì
                    json.Marshal ‚Üí Final JSON

Performance: ~15ms per challenge (200 RPS bottleneck)
```

**New Approach (Fast):**
```
Static Challenge Config ‚Üí protojson.Marshal (at startup) ‚Üí Cached JSON bytes
                                ‚Üì
User Progress Data ‚Üí String formatting ‚Üí Progress fields
                                ‚Üì
            Cached JSON + Injected progress ‚Üí bytes.Buffer
                                ‚Üì
                        Final JSON (zero unmarshal/marshal)

Performance: ~500-800Œºs per challenge (300-350 RPS safe capacity, CPU-limited at 400 RPS)
```

---

### 2.2 Implementation Details

**Files Created:**

**2.2.1 `pkg/cache/serialized_challenge_cache.go` (252 lines)**

Pre-serialization cache that stores static challenge JSON at startup:

```go
type SerializedChallengeCache struct {
    challenges map[string][]byte // challengeID -> JSON
    goals      map[string][]byte // goalID -> JSON
    marshaler  protojson.MarshalOptions
}

func (c *SerializedChallengeCache) WarmUp(challenges []*pb.Challenge) error {
    // Marshal all challenges/goals ONCE at startup
    for _, challenge := range challenges {
        challengeJSON, _ := c.marshaler.Marshal(challenge)
        c.challenges[challenge.ChallengeId] = challengeJSON

        for _, goal := range challenge.Goals {
            goalJSON, _ := c.marshaler.Marshal(goal)
            c.goals[goal.GoalId] = goalJSON
        }
    }
}
```

**2.2.2 `pkg/response/json_injector.go` (365 lines)**

Zero-copy string injection functions:

```go
// Injects progress fields into pre-serialized goal JSON
func InjectProgressIntoGoal(
    staticJSON []byte,
    progress *UserGoalProgress,
) []byte {
    // Find closing brace
    closingBraceIdx := bytes.LastIndexByte(staticJSON, '}')

    // Build progress fields: ,"progress":5,"status":"in_progress",...
    progressFields := buildProgressFields(progress)

    // Insert before closing brace
    result := append(staticJSON[:closingBraceIdx], progressFields...)
    result = append(result, '}')

    return result  // ‚úÖ Zero unmarshal/marshal
}

// Handles nested objects (requirement, reward) within goals
func processGoalsArray(...) error {
    // Parse with depth tracking for nested braces
    depth := 0
    inString := false

    for i := 0; i < len(goalsArrayJSON); i++ {
        c := goalsArrayJSON[i]

        if !inString {
            if c == '{' {
                depth++
            } else if c == '}' {
                depth--
                if depth == 0 {
                    // Found complete goal object
                    goalJSON := goalsArrayJSON[goalStart:i+1]
                    processedGoal := InjectProgressIntoGoal(goalJSON, progress)
                    result.Write(processedGoal)
                }
            }
        }
    }
}
```

**2.2.3 `pkg/response/builder.go` (189 lines)**

Response builder using string injection:

```go
type ChallengeResponseBuilder struct {
    cache *cache.SerializedChallengeCache
}

func (b *ChallengeResponseBuilder) BuildChallengesResponse(
    challengeIDs []string,
    userProgress map[string]*UserGoalProgress,
) ([]byte, error) {
    result := bytes.NewBuffer(make([]byte, 0, len(challengeIDs)*2048+100))
    result.WriteString(`{"challenges":[`)

    for i, challengeID := range challengeIDs {
        // Get pre-serialized JSON from cache
        staticJSON, _ := b.cache.GetChallengeJSON(challengeID)

        // Inject user progress (FAST - no unmarshal/marshal!)
        challengeWithProgress, _ := InjectProgressIntoChallenge(staticJSON, userProgress)

        if i > 0 {
            result.WriteByte(',')
        }
        result.Write(challengeWithProgress)
    }

    result.WriteString(`]}`)
    return result.Bytes(), nil
}
```

**2.2.4 `pkg/handler/optimized_challenges_handler.go`**

HTTP handler using optimized builder:

```go
type OptimizedChallengesHandler struct {
    responseBuilder *response.ChallengeResponseBuilder
    repository      repository.GoalRepository
}

func (h *OptimizedChallengesHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    // Get user progress from DB
    userProgress, _ := h.repository.GetUserProgress(userID, challengeIDs)

    // Build response with string injection (FAST!)
    responseJSON, _ := h.responseBuilder.BuildChallengesResponse(challengeIDs, userProgress)

    w.Header().Set("Content-Type", "application/json")
    w.Write(responseJSON)
}
```

---

### 2.3 Bug Fixes During Implementation

**2.3.1 Nested Object Parsing Bug**

**Problem:** Initial implementation failed to handle nested `requirement` and `reward` objects within goals

**Error Log:**
```
Failed to build optimized response: failed to extract goal_id from goal 0: goal_id field not found
```

**Root Cause:**
```go
// Original (WRONG - hits first } from nested object):
for i := 0; i < len(goalsArrayJSON); i++ {
    if goalsArrayJSON[i] == '{' {
        goalStart = i
    } else if goalsArrayJSON[i] == '}' {
        goalJSON := goalsArrayJSON[goalStart:i+1]  // ‚ùå Incomplete goal
    }
}
```

**Fixed:**
```go
// Corrected (tracks depth for nested braces):
depth := 0
for i := 0; i < len(goalsArrayJSON); i++ {
    if !inString {
        if c == '{' {
            if depth == 0 { goalStart = i }
            depth++
        } else if c == '}' {
            depth--
            if depth == 0 {
                goalJSON := goalsArrayJSON[goalStart:i+1]  // ‚úÖ Complete goal
            }
        }
    }
}
```

**Impact:** Fixed "goal_id field not found" errors when goals have nested objects

---

### 2.4 Performance Results

**Load Test Configuration:**
- Tool: k6
- Duration: 30 seconds
- Virtual Users: 1
- Throughput: 72 RPS actual

**Metrics:**
| Metric | Value |
|--------|-------|
| **Throughput** | 72 RPS (2,162 requests in 30s) |
| **p50 Latency** | 1.98ms |
| **p90 Latency** | 3.24ms |
| **p95 Latency** | 4.04ms |
| **Error Rate** | 0.00% ‚úÖ |
| **Data Received** | 380 MB (13 MB/s) |

**CPU Profile Analysis:**

**Before (Baseline @ 200 RPS):**
```
protojson.marshalMessage:  13.87s (46.39% CPU)  ‚ùå BOTTLENECK
gRPC Handler:               8.18s (27.36%)
Database:                   3.05s (10.20%)
Mappers:                    1.50s ( 5.01%)
```

**After (String Injection @ 72 RPS):**
```
BuildChallengesResponse:    6.15s (41.61%)  ‚Üê String operations
InjectProgressIntoChallenge: 5.48s (37.08%)  ‚Üê String injection
processGoalsArray:          2.58s (17.46%)  ‚Üê Character parsing
protojson.marshalMessage:   NOT FOUND  ‚úÖ ELIMINATED
GC/Runtime:                 3.99s (27.00%)
Network I/O:                2.83s (19.15%)
```

**Verification:**
```bash
$ go tool pprof -list marshalMessage /tmp/cpu_string_injection.prof
no matches found for regexp: marshalMessage  ‚úÖ
```

**CPU Savings:**
- JSON operations: 13.87s ‚Üí 6.15s
- **Reduction: 7.72s (55.6%)**
- **Actual capacity: 200 RPS @ 101% CPU ‚Üí 300-350 RPS @ 65-75% CPU (safe), 400 RPS @ 101% CPU (saturated)**

---

### 2.5 Testing Coverage

**Unit Tests (8 test cases, all passing):**

**File:** `pkg/response/json_injector_test.go`

1. `TestInjectProgressIntoGoal_NoProgress` - Default progress values
2. `TestInjectProgressIntoGoal_WithProgress` - Active progress injection
3. `TestInjectProgressIntoGoal_Completed` - Completed state with timestamps
4. `TestInjectProgressIntoGoal_ComplexJSON` - Nested objects handling
5. `TestInjectProgressIntoChallenge` - Multiple goals
6. `TestInjectProgressIntoChallenge_NoGoals` - Empty goals array
7. `TestInjectProgressIntoChallenge_MissingProgress` - Partial progress data
8. `TestExtractGoalID` - Goal ID extraction edge cases

**Test Results:**
```bash
$ go test ./pkg/response/... -v
=== RUN   TestInjectProgressIntoGoal_NoProgress
--- PASS: TestInjectProgressIntoGoal_NoProgress (0.00s)
...
PASS
ok      extend-challenge-service/pkg/response   0.010s
```

**Benchmark Results:**
```
BenchmarkInjectProgressIntoGoal:        442 ns/op, 544 B/op, 3 allocs/op
BenchmarkInjectProgressIntoChallenge:  3276 ns/op, 2266 B/op, 16 allocs/op
```

**Load Test Results:**
- Duration: 30s
- Requests: 2,162
- Success rate: 100%
- p95 latency: 4.04ms

---

## Failed Attempts & Lessons Learned

### Attempt 1: Pre-Serialization with Unmarshal/Marshal ‚ùå

**Date:** 2025-10-24
**Approach:** Cache pre-serialized JSON, then unmarshal/merge/marshal with progress

**Why It Failed:**
```go
// Cached static JSON
staticJSON := cache.GetChallengeJSON(challengeID)

// Unmarshal to map
var challenge map[string]interface{}
json.Unmarshal(staticJSON, &challenge)  // ‚ùå Still expensive

// Merge progress
mergeProgress(challenge, userProgress)

// Marshal back
json.Marshal(challenge)  // ‚ùå Still expensive
```

**Problem:** Unmarshal + Marshal still consumed 30% CPU. Pre-serialization helped, but not enough.

**Lesson:** Need to avoid JSON parsing entirely, not just cache static data.

---

### Attempt 2: Object Pooling ‚ùå

**Date:** 2025-10-24
**Approach:** Reuse protobuf objects with sync.Pool

**Why It Failed:** (See Phase 1.4 above)

---

### Attempt 3: Use Enum Numbers ‚ùå

**Approach:** Enable `UseEnumNumbers: true` in protojson marshaler

**Why It Failed:** Breaking change for demo app

**Impact:** Would save 5% CPU, but requires updating demo app to handle numeric enums

**Status:** Deferred to future release

---

# Part 2: Event Handler (gRPC Events) Optimization

---

## Event Handler Phase 1: Backpressure Investigation

**Date:** 2025-10-27
**Goal:** Establish baseline and understand failure modes at 500 EPS
**Result:** ‚úÖ Identified PostgreSQL as bottleneck, 48% data loss

### Problem Discovery

Initial load test at 500 EPS revealed catastrophic data loss:

**Test Configuration:**
- Target: 500 EPS constant arrival rate
- Duration: 10 minutes
- Expected events: ~300,000
- Actual processed: 143,424 events (47.8%)
- **Data loss: 52.2%**

### Root Cause Analysis

**CPU Profile Analysis:**

```
pq.(*conn).send:           7.98s (32.59%)  ‚Üê PostgreSQL wire protocol
pq.(*conn).recv:           4.69s (19.15%)  ‚Üê PostgreSQL reads
BatchUpsertProgress:       3.05s (12.46%)  ‚Üê Database operation
```

**Key Findings:**

1. **PostgreSQL CPU Saturated:** 100%+ CPU utilization on 2-core container
2. **Flush Time Too High:** 62-105ms per flush (target: <20ms)
3. **Buffer Overflow:** 47,990 backpressure activations in 10 minutes
4. **Single INSERT Performance:** ~15-20ms per record (1000 records = 15-20 seconds)

### Backpressure Mechanism

The buffered repository implements a 3-tier backpressure system:

```go
// extend-challenge-event-handler/internal/buffered/buffered_repository.go
const (
    maxBufferSize         = 1000  // Normal capacity
    backpressureThreshold = 1500  // Start blocking (150%)
    circuitBreakerLevel   = 2500  // Drop events (250%)
)
```

**Behavior at 500 EPS:**
- Buffer filled to 1000 in ~4 seconds
- Backpressure activated (blocks event processing)
- Flush took 62-105ms but 500 events arrived during that time
- Buffer never drained ‚Üí continuous backpressure ‚Üí drops

**Latency Impact:**
- P95 latency: 10,000ms (circuit breaker timeout)
- P99 latency: 10,000ms
- Average latency: 5,177ms

### Key Metrics (Phase 1 Baseline)

| Metric | Value |
|--------|-------|
| **Throughput** | 239.04 EPS (actual) |
| **Target Rate** | 500 EPS |
| **Success Rate** | 52.0% |
| **Data Loss** | 156,576 events (48%) |
| **Backpressure Activations** | 47,990 times |
| **PostgreSQL CPU** | 100%+ (saturated) |
| **Event Handler CPU** | 18% (waiting on DB) |
| **Flush Time** | 62-105ms |
| **P95 Latency** | 10,000ms |

### Decision: PostgreSQL COPY Protocol

The root cause was clear: **single INSERT statements are too slow for bulk operations**.

PostgreSQL's COPY protocol can insert 1,000 records in 10-20ms (vs 15-20 seconds with single INSERTs).

**Expected improvement:** 5-10x faster flush times ‚Üí 500+ EPS capacity

---

## Event Handler Phase 2: PostgreSQL COPY Protocol

**Date:** 2025-10-28
**Goal:** Implement COPY protocol to eliminate PostgreSQL bottleneck
**Result:** ‚úÖ 99.99% success rate, but k6 bottleneck discovered (195 EPS actual)

### Implementation: Temp Table Pattern

**File:** `extend-challenge-event-handler/internal/repository/postgres_repository.go`

```go
func (r *postgresRepository) BatchUpsertProgressWithCOPY(
    ctx context.Context,
    records []domain.UserGoalProgress,
) error {
    tx, _ := r.db.BeginTx(ctx, nil)
    defer tx.Rollback()

    // Step 1: Create temporary table (in memory, session-scoped)
    _, err := tx.ExecContext(ctx, `
        CREATE TEMP TABLE temp_user_goal_progress (
            user_id VARCHAR(100),
            goal_id VARCHAR(100),
            challenge_id VARCHAR(100),
            namespace VARCHAR(100),
            progress INT,
            status VARCHAR(20),
            completed_at TIMESTAMP,
            claimed_at TIMESTAMP,
            updated_at TIMESTAMP
        ) ON COMMIT DROP
    `)

    // Step 2: COPY data into temp table (FAST - binary protocol)
    stmt, _ := tx.PrepareContext(ctx, pq.CopyIn("temp_user_goal_progress",
        "user_id", "goal_id", "challenge_id", "namespace",
        "progress", "status", "completed_at", "claimed_at", "updated_at"))

    for _, record := range records {
        stmt.Exec(
            record.UserID, record.GoalID, record.ChallengeID,
            record.Namespace, record.Progress, record.Status,
            record.CompletedAt, record.ClaimedAt, record.UpdatedAt,
        )
    }
    stmt.Exec()  // Finalize COPY
    stmt.Close()

    // Step 3: Merge temp ‚Üí permanent (single UPDATE + INSERT)
    _, err = tx.ExecContext(ctx, `
        INSERT INTO user_goal_progress (
            user_id, goal_id, challenge_id, namespace,
            progress, status, completed_at, claimed_at,
            created_at, updated_at
        )
        SELECT
            user_id, goal_id, challenge_id, namespace,
            progress, status, completed_at, claimed_at,
            NOW(), updated_at
        FROM temp_user_goal_progress
        ON CONFLICT (user_id, goal_id)
        DO UPDATE SET
            progress = EXCLUDED.progress,
            status = EXCLUDED.status,
            completed_at = EXCLUDED.completed_at,
            updated_at = EXCLUDED.updated_at
        WHERE user_goal_progress.status != 'claimed'
    `)

    return tx.Commit()
}
```

**Why This Works:**

1. **COPY Protocol:** Binary format, minimal parsing overhead
2. **Temp Table:** In-memory, no WAL logging, no indexes, no constraints
3. **Single UPSERT:** One statement to merge all records
4. **Transaction Isolation:** Atomic operation, no partial updates

### Phase 2 Load Test Results (k6 Bottleneck)

**Test Configuration:**
- Target: 500 EPS
- Duration: 10 minutes
- Expected: ~300,000 events
- **Actual: 117,070 events (39% - k6 issue)**

**Key Metrics:**

| Metric | Phase 1 | Phase 2 | Improvement |
|--------|---------|---------|-------------|
| **Actual EPS** | 239 EPS | 195 EPS | ‚ùå -18% (k6 issue) |
| **Success Rate** | 52.0% | **99.99%** | ‚úÖ +48pp |
| **Data Loss** | 156,576 (48%) | **14 events (0.01%)** | ‚úÖ 99.99% reduction |
| **PostgreSQL CPU** | 100% | **67%** | ‚úÖ -33% |
| **Flush Time (median)** | 62-105ms | **54ms** | ‚úÖ -13% to -49% |
| **Backpressure** | 47,990 | **0** | ‚úÖ 100% eliminated |
| **P95 Latency** | 10,000ms | **302ms** | ‚úÖ 97% faster |

**Critical Discovery: k6 Was the Bottleneck**

Analysis of k6 logs revealed the real problem:

```
WARN[0612] Request Failed    error="dial tcp [...]: connect: cannot assign requested address"
```

**Root Cause:** k6 was creating/destroying connections for every request
- Ran out of ephemeral ports (~28,000 available)
- Test aborted at 195 EPS despite system capacity for 308+ EPS

### k6 Fix: Connection Reuse

**File:** `test/k6/scenario2_event_load.js`

```javascript
// BEFORE (k6 creates new connection every iteration)
export default function() {
    grpc.connect('host.docker.internal:8082', { plaintext: true });
    // ... send event ...
    // Connection destroyed at end of iteration
}

// AFTER (reuse connection across iterations)
let client;
export default function() {
    if (!client) {
        client = grpc.connect('host.docker.internal:8082', {
            plaintext: true,
        });
    }
    // ... send event using persistent connection ...
}
```

### Phase 2 Retest Results (After k6 Fix)

**Test Configuration:**
- Target: 500 EPS
- Duration: 10 minutes
- Expected: ~300,000 events

**Key Metrics:**

| Metric | Value |
|--------|-------|
| **Actual EPS** | **308.87 EPS** (62% of target) |
| **Success Rate** | **100%** |
| **Total Events** | 185,326 events |
| **Buffer Overflows** | 2.98M warnings |
| **PostgreSQL CPU** | 100%+ (still bottlenecked) |
| **Event Handler CPU** | 20% |
| **Flush Time** | 40-50ms |
| **P95 Latency** | 302ms |
| **P99 Latency** | 437ms |

**Analysis:**

‚úÖ **Success:** COPY protocol works, zero data loss
‚ùå **Bottleneck Persists:** PostgreSQL still at 100% CPU
üîç **Next Step:** Phase 3 - Increase flush frequency (1000ms ‚Üí 100ms)

---

## Event Handler Phase 3: Flush Interval Tuning

**Date:** 2025-10-28
**Goal:** Break through 308 EPS ceiling by flushing 10x more frequently
**Result:** ‚ö†Ô∏è No improvement - PostgreSQL still bottleneck (308 EPS)

### Configuration Changes

**File:** `extend-challenge-event-handler/main.go:185-191`

| Parameter | Phase 2 Value | Phase 3 Value | Change |
|-----------|--------------|---------------|--------|
| **Flush Interval** | 1000ms | **100ms** | **10x more frequent** |
| **Max Buffer Size** | 1,000 records | **3,000 records** | **3x larger** |
| **Backpressure Threshold** | 1,500 records | **4,500 records** | **3x higher** |
| **Circuit Breaker** | 2,500 records | **7,500 records** | **3x higher** |

```go
// BEFORE (Phase 2)
bufferedRepo := buffered.NewBufferedRepository(
    postgresRepo,
    goalCache,
    namespace,
    1*time.Second,  // Flush every 1 second
    1000,           // Buffer 1,000 records
    zerologLogger,
)

// AFTER (Phase 3)
bufferedRepo := buffered.NewBufferedRepository(
    postgresRepo,
    goalCache,
    namespace,
    100*time.Millisecond, // Flush every 100ms (10x more frequent)
    3000,                 // Buffer 3,000 records (3x larger)
    zerologLogger,
)
```

### Theoretical Analysis

**Phase 2 Bottleneck:**

```
Flush performance:  40-50ms per flush
Records per flush:  1,000 records max
Flush interval:     1 second (timer-based)

At 308 EPS ingestion rate:
  Buffer fills in:  1,000 / 308 = 3.25 seconds
  System flushes:   Every 1 second (timer triggers first)
  Records per flush: ~308 records (1 second √ó 308 EPS)

Problem: Buffer never fills to 1,000, so size-based flush never triggers!
Result: Throughput limited by timer interval (1s), not flush performance (50ms).
```

**Phase 3 Expected:**

```
At 500 EPS target rate:
  Events per 100ms:  50 events (500 / 10)
  System flushes:    Every 100ms (timer triggers)
  Records per flush: ~50 records (100ms √ó 500 EPS)

Expected result:
  - 10 flushes per second (was 1)
  - 50 records per flush (was 308)
  - Each flush takes 40-50ms (same as before)
  - Idle time: 50-60ms per flush cycle (vs 950ms before)
  - Throughput ceiling: 500+ EPS ‚úÖ
```

### Phase 3 Load Test Results

**Test Configuration:**
- Target: 500 EPS
- Duration: 10 minutes
- Expected: ~300,000 events

**Key Metrics:**

| Metric | Phase 2 | Phase 3 | Change |
|--------|---------|---------|--------|
| **Actual EPS** | 308.87 | 308.87 | ‚ùå No improvement |
| **Success Rate** | 100% | 100% | ‚úÖ Maintained |
| **Buffer Overflows** | 0 warnings | 2.98M warnings | ‚ùå Worse |
| **PostgreSQL CPU** | 100%+ | 100%+ | ‚ùå Still bottleneck |
| **Flush Time** | 40-50ms | 40-50ms | ‚û°Ô∏è Unchanged |
| **Flushes/sec** | 1 | 10 | ‚úÖ 10x as planned |
| **Records/flush** | ~308 | ~31 | ‚úÖ 10x smaller |

**Analysis:**

‚ùå **Failed to improve throughput:** Still limited to 308 EPS
‚úÖ **Achieved flush frequency goal:** 10 flushes/sec
‚ùå **PostgreSQL still bottleneck:** 100%+ CPU, can't process more data
üîç **Root Cause:** Docker environment with HDD-backed PostgreSQL

**Decision:** Test with more PostgreSQL CPUs (Phase 3.5) before implementing parallel workers

---

## Event Handler Phase 3.5: PostgreSQL 4 CPUs

**Date:** 2025-10-28
**Goal:** Eliminate PostgreSQL bottleneck by doubling CPU allocation
**Result:** ‚ö†Ô∏è 95% of target (475 EPS), but latency exploded (10s P95)

### Configuration Changes

**File:** `docker-compose.yml`

```yaml
# BEFORE (Phase 3)
challenge-postgres:
  cpus: '2.0'

# AFTER (Phase 3.5)
challenge-postgres:
  cpus: '4.0'  # Doubled CPU allocation
```

### Load Test Results

**Test Configuration:**
- Target: 500 EPS
- Duration: 10 minutes
- Expected: ~300,000 events

**Key Metrics:**

| Metric | Phase 3 (2 CPUs) | Phase 3.5 (4 CPUs) | Improvement |
|--------|------------------|-------------------|-------------|
| **Actual EPS** | 308.87 | **475.30** | ‚úÖ +54% |
| **Success Rate** | 100% | 100% | ‚úÖ Maintained |
| **PostgreSQL CPU** | 100%+ (2 cores) | **50%** (4 cores) | ‚úÖ Eliminated bottleneck |
| **Event Handler CPU** | 20% | **25%** | ‚û°Ô∏è Slightly higher |
| **P95 Latency** | 302ms | **10,000ms** | ‚ùå 33x worse! |
| **P99 Latency** | 437ms | **10,000ms** | ‚ùå 23x worse! |
| **Avg Latency** | ~50ms | **1,650ms** | ‚ùå 33x worse! |

**Critical Finding: New Bottleneck Appeared**

PostgreSQL CPU dropped from 100% to 50%, but latency exploded:

```
Phase 3:   302ms P95 (PostgreSQL bottleneck)
Phase 3.5: 10,000ms P95 (Event Handler bottleneck)
```

**Root Cause:** Single-threaded flush became the bottleneck
- PostgreSQL can now handle more load (4 CPUs)
- Event Handler has only 1 flush goroutine
- Flush queue backed up ‚Üí 10s circuit breaker timeout triggered

**Decision:** Implement parallel flush workers (Phase 4)

---

## Event Handler Phase 4: 8 Parallel Flush Workers

**Date:** 2025-10-28
**Goal:** Eliminate single-threaded flush bottleneck with parallel workers
**Result:** ‚úÖ Excellent latency (44ms P95), but throughput same as Phase 3.5 (474 EPS)

### Architecture: Hash-Based Partitioning

**New Component:** `PartitionedBufferedRepository`

**File:** `extend-challenge-event-handler/internal/buffered/partitioned_buffered_repository.go`

```go
type PartitionedBufferedRepository struct {
    partitions []*BufferedRepository  // 8 independent partitions
    numWorkers int                     // 8 parallel flush workers
}

func NewPartitionedBufferedRepository(
    underlying repository.GoalRepository,
    goalCache cache.GoalCache,
    namespace string,
    flushInterval time.Duration,
    bufferSize int,
    numWorkers int,  // 8
    logger zerolog.Logger,
) *PartitionedBufferedRepository {
    partitions := make([]*BufferedRepository, numWorkers)

    for i := 0; i < numWorkers; i++ {
        partitions[i] = NewBufferedRepository(
            underlying,
            goalCache,
            namespace,
            flushInterval,
            bufferSize,
            logger.With().Int("partition_id", i).Logger(),
        )
    }

    return &PartitionedBufferedRepository{
        partitions: partitions,
        numWorkers: numWorkers,
    }
}

func (r *PartitionedBufferedRepository) UpsertUserGoalProgress(
    ctx context.Context,
    progress *domain.UserGoalProgress,
) error {
    // Hash user_id to partition (consistent hashing)
    h := fnv.New32a()
    h.Write([]byte(progress.UserID))
    partitionIdx := int(h.Sum32()) % r.numWorkers

    // Route to partition (each has its own flush worker)
    return r.partitions[partitionIdx].UpsertUserGoalProgress(ctx, progress)
}
```

**Key Design Decisions:**

1. **Hash Function:** FNV-1a (fast, uniform distribution)
2. **Partition Key:** `user_id` (ensures per-user ordering)
3. **Independent Buffers:** Each partition has its own 3,000-record buffer
4. **Independent Flush Workers:** 8 goroutines flushing in parallel
5. **Configuration:** 100ms flush interval, 3,000 buffer per partition

### Load Test Results

**Test Configuration:**
- Target: 500 EPS
- Duration: 10 minutes
- Expected: ~300,000 events

**Key Metrics:**

| Metric | Phase 3.5 (4 CPUs, 1 Worker) | Phase 4 (4 CPUs, 8 Workers) | Improvement |
|--------|------------------------------|----------------------------|-------------|
| **Actual EPS** | 475.30 | **474.08** | ‚û°Ô∏è Same (95% of target) |
| **Success Rate** | 100% | 100% | ‚úÖ Maintained |
| **P95 Latency** | 10,000ms | **44ms** | ‚úÖ 99.6% faster! |
| **P99 Latency** | 10,000ms | **98ms** | ‚úÖ 99.0% faster! |
| **Avg Latency** | 1,650ms | **11ms** | ‚úÖ 99.3% faster! |
| **Backpressure** | Frequent | **0** | ‚úÖ Eliminated |
| **PostgreSQL CPU** | 50% | **25%** | ‚û°Ô∏è Under-utilized |
| **Event Handler CPU** | 25% | **20%** | ‚û°Ô∏è Lower (efficient) |

**Hash Distribution Analysis:**

```
Partition 0: 5,907 flushes (12.5%), avg 59.7 records/flush
Partition 1: 5,812 flushes (12.3%), avg 60.8 records/flush
Partition 2: 5,813 flushes (12.3%), avg 60.8 records/flush
Partition 3: 5,807 flushes (12.3%), avg 60.8 records/flush
Partition 4: 5,956 flushes (12.6%), avg 59.3 records/flush
Partition 5: 5,797 flushes (12.3%), avg 60.9 records/flush
Partition 6: 5,949 flushes (12.6%), avg 59.4 records/flush
Partition 7: 6,135 flushes (13.0%), avg 57.6 records/flush

Variance: ¬±1.5% (excellent distribution)
```

**Analysis:**

‚úÖ **Latency Success:** 44ms P95 (vs 10,000ms Phase 3.5) - 227x improvement!
‚ùå **Throughput Unchanged:** 474 EPS (same as Phase 3.5)
üîç **New Bottleneck:** k6 load generator again (not enough VUs)

**CPU Utilization:**
- PostgreSQL: 25% (was 50% in Phase 3.5) - under-utilized
- Event Handler: 20% (was 25% in Phase 3.5) - efficient

**Decision:** Increase k6 VUs to fully saturate system (Phase 4b)

---

## Event Handler Phase 4b: k6 Load Generator Fix

**Date:** 2025-10-29
**Goal:** Increase k6 VUs to achieve full 500 EPS throughput
**Result:** ‚úÖ **SUCCESS! 494 EPS (98.7% of target)**

### k6 Configuration Changes

**File:** `test/k6/scenario2_event_load.js`

```javascript
// BEFORE (Phase 4)
export const options = {
    scenarios: {
        constant_load: {
            executor: 'constant-arrival-rate',
            rate: TARGET_EPS,
            timeUnit: '1s',
            duration: TEST_DURATION,
            preAllocatedVUs: 500,     // Not enough!
            maxVUs: 500,              // Capped too low!
        },
    },
};

// AFTER (Phase 4b)
export const options = {
    scenarios: {
        constant_load: {
            executor: 'constant-arrival-rate',
            rate: TARGET_EPS,
            timeUnit: '1s',
            duration: TEST_DURATION,
            preAllocatedVUs: 1000,    // 2x increase
            maxVUs: 1500,             // 3x increase (headroom)
        },
    },
};
```

**Why This Fixed It:**

In Phase 4, k6 ran out of VUs because:
1. Each VU sends event ‚Üí waits for response ‚Üí repeats
2. At 474 EPS with ~44ms latency, need ~22 active VUs
3. But k6 was dropping iterations due to VU exhaustion

Increasing VUs gave k6 enough capacity to maintain 500 EPS target rate.

### Phase 4b Load Test Results (FINAL)

**Test Configuration:**
- Target: 500 EPS
- Duration: 10 minutes
- Expected: ~300,000 events

**Key Metrics:**

| Metric | Phase 4 (500/500 VUs) | Phase 4b (1000/1500 VUs) | Improvement |
|--------|----------------------|--------------------------|-------------|
| **Actual EPS** | 474.08 | **493.68** | ‚úÖ +4.1% (98.7% of target) |
| **Success Rate** | 100% | 100% | ‚úÖ Maintained |
| **Total Events** | 284,453 | 296,211 | ‚úÖ +4.1% |
| **Dropped Iterations** | 15,547 (5.2%) | **3,789 (1.3%)** | ‚úÖ -75% |
| **P95 Latency** | 44ms | **21ms** | ‚úÖ -52% |
| **P99 Latency** | 98ms | **45ms** | ‚úÖ -54% |
| **Avg Latency** | 11ms | **6.4ms** | ‚úÖ -42% |
| **PostgreSQL CPU** | 25% | **23%** | ‚û°Ô∏è Stable |
| **Event Handler CPU** | 20% | **21%** | ‚û°Ô∏è Stable |

**Per-Partition Analysis:**

```
Partition 0: 6,151 flushes, avg 60.1 records/flush
Partition 1: 6,025 flushes, avg 61.6 records/flush
Partition 2: 6,030 flushes, avg 61.5 records/flush
Partition 3: 6,027 flushes, avg 61.5 records/flush
Partition 4: 6,195 flushes, avg 59.9 records/flush
Partition 5: 6,017 flushes, avg 61.6 records/flush
Partition 6: 6,180 flushes, avg 60.0 records/flush
Partition 7: 6,385 flushes, avg 58.0 records/flush

Total flushes: 49,010
Total records: 296,211
Variance: ¬±1.8% (excellent distribution)
```

**Final Achievement:**

üéØ **Target:** 500 EPS
‚úÖ **Result:** **493.68 EPS (98.7% of target)**
‚úÖ **Success Rate:** 100% (zero data loss)
‚úÖ **Latency:** 21ms P95, 45ms P99 (excellent)
‚úÖ **Resource Usage:** 23% PostgreSQL CPU, 21% Event Handler CPU (efficient)

---

# Part 3: Combined Load Testing

---

## Phase 5: Combined Load Testing (300 RPS + 500 EPS)

**Date:** 2025-10-29
**Goal:** Validate system behavior under simultaneous API and event load
**Result:** ‚úÖ 99.95% success rate, but **PostgreSQL becomes primary bottleneck**

### Motivation

All previous phases tested services **independently**:
- Challenge Service: Tested alone at 200-400 RPS
- Event Handler: Tested alone at 500 EPS

**Production reality:** Both services run simultaneously and share the same PostgreSQL database.

This phase validates:
1. How services interact under combined load
2. Whether database can handle both read (API) and write (events) operations
3. Resource contention and scaling limits

### Test Configuration

**Load Profile:**
- **Challenge Service:** 300 RPS (constant-arrival-rate)
- **Event Handler:** 500 EPS (constant-arrival-rate)
- **Duration:** 5 minutes (300 seconds)
- **Configuration:** 500 total goals (10 challenges √ó 50 goals each)

**k6 Configuration (`test/k6/scenario3_combined.js`):**
```javascript
export let options = {
  scenarios: {
    api_load: {
      executor: 'constant-arrival-rate',
      rate: 300,  // 300 RPS
      duration: '5m',
      preAllocatedVUs: 300,
      maxVUs: 600,
      exec: 'apiLoad',
    },
    event_load: {
      executor: 'constant-arrival-rate',
      rate: 500,  // 500 EPS
      duration: '5m',
      preAllocatedVUs: 1000,
      maxVUs: 1500,
      exec: 'eventLoad',
    },
  },
};
```

**Service Resources:**
- Challenge Service: 1 CPU, 1 GB RAM
- Event Handler: 1 CPU, 1 GB RAM (8 flush workers)
- PostgreSQL: **4 CPUs**, 4 GB RAM

### Load Test Results

**Overall Performance:**

| Metric | Value | Status |
|--------|-------|--------|
| **HTTP Requests** | 539,953 (298.59/s) | ‚úÖ 300 RPS target met |
| **HTTP Success Rate** | 100% (0 failures) | ‚úÖ Perfect |
| **HTTP P95 Latency** | **163ms** | ‚úÖ Acceptable |
| **gRPC Events** | 630,574 processed | ‚ö†Ô∏è Some dropped |
| **gRPC Success Rate** | 99.95% (505 failures) | ‚úÖ Good |
| **gRPC P95 Latency** | **20s** | üî¥ Very high (backpressure) |
| **Dropped Iterations** | 268,969 (18.7%) | üî¥ Significant |
| **Total Checks** | 1,171,032 | ‚úÖ |

**HTTP Performance (Challenge Service):**

| Metric | Individual (300 RPS) | Combined (300 RPS) | Degradation |
|--------|---------------------|-------------------|-------------|
| **P50 Latency** | 1.98ms | 49.61ms | **25x worse** |
| **P95 Latency** | 3.63ms | 163.46ms | **45x worse** |
| **Max Latency** | <100ms | 3.03s | **>30x worse** |
| **CPU Usage** | 65% | **93-98%** | Near saturation |

**gRPC Performance (Event Handler):**

| Metric | Individual (494 EPS) | Combined (500 EPS) | Degradation |
|--------|---------------------|-------------------|-------------|
| **Median Latency** | <2ms | 1.09ms | ‚úÖ Similar |
| **P95 Latency** | 21ms | **20s** | **952x worse** |
| **Avg Latency** | 6.4ms | 4.11s | **642x worse** |
| **Dropped Iterations** | 1.3% | **18.7%** | **14x worse** |

### Critical Finding: PostgreSQL Bottleneck

**The system is bottlenecked by PostgreSQL, not the application services.**

**Resource Utilization:**

| Component | CPU Usage | Status |
|-----------|-----------|--------|
| **PostgreSQL** | **406-428%** (4/4 cores) | üî¥ **Maxed out** |
| **Challenge Service** | 93-98% | ‚ö†Ô∏è Near saturation |
| **Event Handler** | 40-50% | ‚úÖ Healthy |

**Database Load Breakdown:**
- **SELECT queries:** 300/sec (API reads)
- **INSERT operations:** ~4,000 rows/sec (8 workers √ó 500 rows/sec)
- **Total:** Mixed read-write workload with lock contention

**Key Observations:**

1. **Read-Write Contention:** API SELECT queries and event INSERT operations compete for locks
2. **Flush Time Variability:** 76-127ms (67% variance) due to lock contention
3. **Backpressure Triggered:** 505 failed events, timeouts occurred when flushes were slow
4. **Correlated CPU Usage:** When PostgreSQL CPU dips, Challenge Service CPU also dips

### CPU Profile Analysis

**Challenge Service (@ 300 RPS combined load):**

```
BuildChallengesResponse:        20.08s (67.47%)  ‚Üê String injection (same as standalone)
InjectProgressIntoChallenge:    17.78s (59.70%)
processGoalsArray:               8.38s (28.15%)
Database queries:                2.87s ( 9.64%)  ‚Üê Slightly slower due to contention
```

**Event Handler (@ 500 EPS combined load):**

```
pq.conn.send (PostgreSQL COPY):  10.91s (36.98%)  ‚Üê PostgreSQL I/O
pq.conn.recv:                      6.53s (22.13%)
BatchUpsertProgress:               5.44s (18.44%)
Backpressure blocking:             4.12s (13.97%)  ‚Üê NEW: lock wait time
```

### Configuration Impact: 500 Goals

**This test validates system capacity with 500 goals (10 challenges √ó 50 goals each).**

The goal count significantly impacts performance across all components:

**Challenge Service Impact (93-98% CPU):**
- **Response Size:** GET /v1/challenges returns all 500 goals per request
- **String Injection:** 67% of CPU time spent injecting progress into 500 goal objects
- **Throughput Math:** 300 RPS √ó 500 goals = **150,000 goal objects serialized/second**
- **Implication:** Doubling goals to 1,000 would likely saturate CPU at current RPS

**Event Handler Impact (40-50% CPU):**
- **Goal Evaluations:** Each event checks against matching goals by `stat_code`
- **Login Events (20%):** 100 EPS √ó 50 matching goals = **5,000 evaluations/sec**
- **Stat Events (80%):** 400 EPS √ó 50 matching goals = **20,000 evaluations/sec**
- **Total:** ~25,000 goal evaluations per second
- **Implication:** Event Handler has headroom; could handle 2-3x more events

**Database Impact (410% CPU):**
- **Row Scaling:** Each active user can have up to 500 rows (one per goal)
- **Lock Contention:** More goals = more rows = worse contention during combined load
- **Implication:** Database is the primary bottleneck regardless of goal count

### Comparison: Individual vs Combined Load

**Superlinear Degradation Due to Lock Contention:**

| Scenario | PostgreSQL CPU | HTTP P95 | gRPC P95 | Notes |
|----------|----------------|----------|----------|-------|
| **API Only** (300 RPS) | ~10% | 3.63ms | N/A | Low database load |
| **Events Only** (500 EPS) | ~25% | N/A | 21ms | COPY protocol efficient |
| **Combined** (300 RPS + 500 EPS) | **410%** | 163ms | 20s | üî¥ Lock contention! |

**Expected vs Actual:**
- **Expected:** 10% + 25% = 35% PostgreSQL CPU
- **Actual:** 410% PostgreSQL CPU
- **Degradation:** **11.7x worse** than linear sum (due to lock contention)

**Why Superlinear Degradation Occurs:**

1. **SELECT queries block on INSERT locks**
2. **INSERT operations wait for SELECT to complete**
3. **Both operations compete for same table locks**
4. **Result:** Cascading delays, queue buildup, timeouts

### Backpressure Analysis

**Event Handler Backpressure System:**

```go
const (
    maxBufferSize         = 3000   // Per partition
    backpressureThreshold = 4500   // Block event processing
    circuitBreakerLevel   = 7500   // Drop events (timeout)
)
```

**Observed Behavior:**
- **505 failed events** (0.04% of total)
- **Timeout:** 20s circuit breaker triggered when PostgreSQL slow
- **Flush Time Variance:** 76-127ms (67% variance)
- **Root Cause:** PostgreSQL lock contention causing variable flush times

**Timeline of Backpressure Activation:**

```
1. PostgreSQL at 400%+ CPU (lock contention)
   ‚Üì
2. Flush takes 120ms instead of 80ms
   ‚Üì
3. During that 40ms delay, 20 more events arrive (500 EPS)
   ‚Üì
4. Buffer fills beyond threshold
   ‚Üì
5. Backpressure blocks event processing
   ‚Üì
6. If flush completes: Buffer drains, normal operation resumes
7. If flush still slow: Circuit breaker timeout (20s), drop event
```

### Capacity Planning Guidelines

**Tested Configuration (1 CPU per service, 4 CPUs PostgreSQL):**
```
500 goals ‚Üí 300 RPS + 500 EPS = 99.95% success (PostgreSQL bottlenecked)
```

**Projected Scaling:**

**Goal Count Impact:**
- **‚â§500 goals:** Current architecture handles well with proper database scaling
- **500-1,000 goals:** Challenge Service would saturate at ~150-200 RPS (string injection bottleneck)
- **1,000-2,000 goals:** Would require API pagination or filtering
- **>2,000 goals:** Must redesign API (filtered/paginated responses, not all goals)

**Database Scaling Requirements:**

| Load | PostgreSQL CPUs | Expected Result |
|------|----------------|-----------------|
| 300 RPS + 500 EPS | 4 CPUs | üî¥ 410% usage, backpressure |
| 300 RPS + 500 EPS | **8 CPUs** | ‚úÖ ~50% usage, no contention |
| 600 RPS + 1,000 EPS | 8 CPUs | ‚ö†Ô∏è ~100% usage |
| 600 RPS + 1,000 EPS | **16 CPUs** | ‚úÖ ~50% usage |

**Horizontal Scaling (with 8-CPU PostgreSQL):**
- Challenge Service: 3 pods √ó 300 RPS = 900 RPS
- Event Handler: 2 pods √ó 500 EPS = 1,000 EPS
- **Combined Capacity:** 900 RPS + 1,000 EPS (assumes read replicas for API)

### Scaling Recommendations

**Immediate Actions (Before Production):**

1. **‚úÖ Scale PostgreSQL to 8+ CPUs**
   - Eliminates lock contention bottleneck
   - Reduces flush time variance from 67% to <10%
   - Enables 600-900 RPS + 1,000 EPS capacity

2. **‚úÖ Add Read Replicas for API Queries**
   - Offload SELECT queries to read replicas
   - Eliminates read-write contention
   - Primary handles only write operations (events)
   - Can scale reads independently of writes

3. **‚ö†Ô∏è Monitor Challenge Service CPU at Production Load**
   - Currently at 93-98% with 500 goals √ó 300 RPS
   - Scale horizontally if sustained above 80%
   - Consider goal count limits (<500 per deployment)

**Short-Term Optimizations:**

1. **PostgreSQL Tuning:**
   ```sql
   work_mem = 16MB              -- For sorting/grouping
   shared_buffers = 2GB         -- For caching (50% of RAM)
   effective_cache_size = 6GB   -- For query planner
   synchronous_commit = off     -- For faster writes (accept small data loss risk)
   ```

2. **Connection Pooling:**
   ```go
   db.SetMaxOpenConns(50)       // Limit connections
   db.SetMaxIdleConns(25)       // Reuse connections
   db.SetConnMaxLifetime(5m)    // Recycle connections
   ```

3. **Challenge Service Optimization:**
   - Implement response caching for anonymous users (Redis)
   - Reduce goal count per challenge (<50 goals)
   - Consider pagination for GET /v1/challenges

**Production Deployment (Recommended):**

```yaml
# Challenge Service
replicas: 3
resources:
  limits:
    cpu: "1"
    memory: "256Mi"
autoscaling:
  targetCPUUtilizationPercentage: 70  # Scale at 300 RPS/pod

# Event Handler
replicas: 2
resources:
  limits:
    cpu: "0.5"
    memory: "256Mi"
autoscaling:
  targetCPUUtilizationPercentage: 60  # Scale at 400 EPS/pod

# PostgreSQL (PRIMARY)
resources:
  limits:
    cpu: "8"          # 8 CPUs for combined load
    memory: "8Gi"
  requests:
    cpu: "4"
    memory: "4Gi"
config:
  max_connections: 200
  shared_buffers: "2GB"
  work_mem: "16MB"

# PostgreSQL (READ REPLICAS)
replicas: 2           # For API read scaling
resources:
  limits:
    cpu: "4"
    memory: "4Gi"
```

**Expected Production Capacity:**
- API: 900 RPS (3 Challenge Service pods)
- Events: 800-1,000 EPS (2 Event Handler pods)
- Database: 8 CPUs primary + 2√ó4 CPUs read replicas
- **Total:** 900 RPS + 1,000 EPS with <50% database CPU

### Key Takeaways

**‚úÖ Successes:**
1. **99.95% success rate** under combined load
2. **Application services perform well** (optimizations effective)
3. **Backpressure system works** (only 505 failures despite heavy contention)
4. **500-goal configuration validated** (realistic production scenario)

**üî¥ Critical Findings:**
1. **PostgreSQL is the primary bottleneck** (not application code)
2. **Lock contention causes superlinear degradation** (11.7x worse than expected)
3. **Database scaling is mandatory** before production (4 CPUs ‚Üí 8+ CPUs)
4. **Read replicas strongly recommended** to eliminate read-write contention

**üìä Performance at Safe Capacity:**
- **Current (4 CPU PostgreSQL):** 300 RPS + 500 EPS = 99.95% success (PostgreSQL maxed)
- **With 8 CPU PostgreSQL:** 600 RPS + 1,000 EPS = 100% success (projected)
- **With Read Replicas:** 900 RPS + 1,000 EPS = 100% success (projected)

---

## Event Handler Architecture

### Final System Design

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Event Ingestion                          ‚îÇ
‚îÇ  (k6 ‚Üí gRPC ‚Üí Event Handler receives 500 EPS target)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Event Processing Pipeline   ‚îÇ
              ‚îÇ  - Validate event            ‚îÇ
              ‚îÇ  - Lookup affected goals     ‚îÇ
              ‚îÇ  - Calculate progress        ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Hash-Based Partitioning      ‚îÇ
              ‚îÇ FNV-1a(user_id) % 8          ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚ñº                   ‚ñº                   ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Part 0 ‚îÇ          ‚îÇ Part 1 ‚îÇ   ...    ‚îÇ Part 7 ‚îÇ
    ‚îÇ 3K buf ‚îÇ          ‚îÇ 3K buf ‚îÇ          ‚îÇ 3K buf ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚îÇ Flush             ‚îÇ Flush             ‚îÇ Flush
        ‚îÇ 100ms             ‚îÇ 100ms             ‚îÇ 100ms
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚ñº                   ‚ñº                   ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇWorker 0‚îÇ          ‚îÇWorker 1‚îÇ   ...    ‚îÇWorker 7‚îÇ
    ‚îÇ COPY   ‚îÇ          ‚îÇ COPY   ‚îÇ          ‚îÇ COPY   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   PostgreSQL (4 CPUs)        ‚îÇ
              ‚îÇ   COPY Protocol              ‚îÇ
              ‚îÇ   Temp Table Pattern         ‚îÇ
              ‚îÇ   23% CPU utilization        ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

**1. Event Processing Pipeline**

File: `extend-challenge-event-handler/internal/processor/event_processor.go`

- Receives events via gRPC from Kafka (abstracted by Extend platform)
- Validates event schema and namespace
- Looks up affected goals from in-memory cache
- Calculates progress deltas

**2. Partitioned Buffered Repository**

File: `extend-challenge-event-handler/internal/buffered/partitioned_buffered_repository.go`

- 8 independent partitions with hash-based routing
- Each partition: 3,000-record buffer, 100ms flush interval
- Per-user mutex within partitions (prevents race conditions)
- Map-based deduplication (only latest progress per user-goal pair)

**3. PostgreSQL COPY Protocol**

File: `extend-challenge-event-handler/internal/repository/postgres_repository.go`

- Temp table pattern (in-memory, session-scoped)
- Binary COPY protocol (fast bulk insert)
- Single UPSERT statement to merge all records
- Atomic transactions (all-or-nothing)

**4. Backpressure System**

File: `extend-challenge-event-handler/internal/buffered/buffered_repository.go`

```go
const (
    maxBufferSize         = 3000  // Normal capacity per partition
    backpressureThreshold = 4500  // Start blocking (150%)
    circuitBreakerLevel   = 7500  // Drop events (250%)
)
```

With 8 partitions and efficient COPY protocol, backpressure is never triggered at 500 EPS.

### Performance Characteristics

**Flush Performance:**
- Median: 40-50ms per partition
- 8 partitions flushing in parallel
- ~60 records per flush (500 EPS / 10 flushes/sec / 8 partitions)

**Resource Usage @ 494 EPS:**
- Event Handler CPU: 21%
- PostgreSQL CPU: 23%
- Memory: Stable (<100 MB)
- Network: ~12 MB/s

**Scalability Limits:**
- Current capacity: 494 EPS (98.7% of 500 EPS target)
- Theoretical max: ~800 EPS (based on CPU headroom)
- Bottleneck: k6 VU allocation (can be increased further)

---

## Current System Architecture

### Request Flow (Optimized)

```
1. HTTP Request ‚Üí gRPC Gateway ‚Üí Optimized Handler
                                    ‚Üì
2. Get challengeIDs from config (in-memory)
                                    ‚Üì
3. Get userProgress from PostgreSQL (single query)
                                    ‚Üì
4. Response Builder:
   a. Get cached challenge JSON for each ID
   b. Inject user progress via string manipulation
   c. Concatenate into {"challenges":[...]} structure
                                    ‚Üì
5. Return JSON bytes (zero protojson marshaling)
```

**Key Components:**

1. **SerializedChallengeCache** (`pkg/cache/`)
   - Warms up at startup
   - Stores ~259 KB of pre-serialized JSON
   - Thread-safe (RWMutex)

2. **ChallengeResponseBuilder** (`pkg/response/`)
   - Uses cached JSON
   - Injects progress via strings
   - ~500-800Œºs per challenge

3. **OptimizedChallengesHandler** (`pkg/handler/`)
   - Registered at `/challenge/v1/challenges`
   - Bypasses standard gRPC-Gateway (direct HTTP handler)
   - Uses ChallengeResponseBuilder

4. **Standard gRPC Handlers** (all other endpoints)
   - Still use protojson marshaling
   - Plan to optimize in future sprints

---

## Performance Characteristics

### Throughput Scaling

| RPS | CPU Usage | Memory | Latency (p95) | Status |
|-----|-----------|--------|---------------|---------|
| 100 | ~25% | 24 MB | 2-3ms | ‚úÖ Comfortable |
| 200 | ~45% | 24 MB | 4.04ms | ‚úÖ Optimal |
| 300 | **65%** | **24 MB** | **3.63ms** | ‚úÖ Safe |
| 350 | ~75% | ~50 MB (est) | 5-6ms (est) | ‚ö†Ô∏è Recommended max |
| **400** | **101%** | **83 MB** | **1,442ms** | ‚ùå **CPU SATURATED** |
| 450+ | Overloaded | 100+ MB | 2000ms+ | ‚ùå Unacceptable |

**Measured Data:**
- **300 RPS:** 217s test, 69,895 requests (322 actual RPS sustained)
- **400 RPS:** 1,592s test, CPU saturated at 101%, **latency degraded 397x** (3.63ms ‚Üí 1,442ms)

**Critical Finding:** Beyond ~350 RPS, the service hits CPU saturation and latency degrades catastrophically. The "cliff" occurs between 350-380 RPS.

**Memory Scaling:** Memory usage increases slowly with RPS (24 MB @ 300 RPS ‚Üí 83 MB @ 400 RPS) but remains negligible compared to CPU constraints.

### Resource Usage (@ 200 RPS)

**CPU Breakdown:**
- String Injection: 41.61%
- GC/Runtime: 27.00%
- Network I/O: 19.15%
- Database: ~5-10%
- Other: ~2-8%

**Memory Breakdown:**
- Heap in-use: 24 MiB / 2 GiB (1.2%)
- Cached JSON: ~259 KB (negligible)
- Active connections: minimal
- Available headroom: 1.98 GB

### Resource Usage (@ 300 RPS)

**CPU Breakdown:**
- BuildChallengesResponse: 40.71%
- InjectProgressIntoChallenge: 35.90%
- processGoalsArray: 27.29%
- GC/Runtime: 26.64%
- Network I/O (syscalls): 19.49%
- Database: ~5-10%

**Memory Breakdown:**
- Heap in-use: 24 MiB / 2 GiB (1.2%)
- Top allocators:
  - bufio readers/writers: 2.5 MB
  - String injection: 520 KB
  - Protobuf formatting: 520 KB
- Cached JSON: ~259 KB
- Available headroom: 1.98 GB

**Key Finding:** Memory usage remains constant at ~24 MB regardless of RPS. The optimization achieved **~95% memory reduction** compared to initial estimates (24 MB vs 600 MB @ 300 RPS).

### Resource Usage (@ 400 RPS) - **CPU SATURATED**

**CPU Breakdown:**
- BuildChallengesResponse: 57.48%
- InjectProgressIntoChallenge: 50.10%
- processGoalsArray: 36.80%
- Network I/O (syscalls): 18.03%
- Memory allocation: 16.33%
- **Total utilization: 97.51%** (saturated)

**Memory Breakdown:**
- Heap in-use: 20.9 MiB (still only 1% of 2GB)
- Top allocators:
  - bytes.growSlice: 6.4 MB (buffer growth under pressure)
  - InjectProgressIntoChallenge: 4.7 MB
  - InjectProgressIntoGoal: 2.0 MB
  - bufio writers: 2.6 MB

**Performance Degradation @ 400 RPS:**
- CPU: 101% (saturated)
- Container memory: 83 MB (3.5x increase from 300 RPS)
- Latency p95: **1,442ms** (397x worse than 300 RPS!)
- Latency p99: **1,977ms** (365x worse than 300 RPS!)

**Critical Finding:** At 400 RPS, CPU saturation causes request queuing and catastrophic latency degradation. The service becomes unusable beyond ~380 RPS.

---

# Part 4: Deployment & Conclusion

---

## Deployment Recommendations

### Challenge Service Production Configuration

**Container Resources:**
```yaml
challenge-service:
  resources:
    limits:
      cpu: "1"          # 1 core for ~350 RPS safe max
      memory: "256Mi"   # 3x headroom (83 MB max observed @ 400 RPS)
    requests:
      cpu: "0.5"        # Baseline for 200+ RPS
      memory: "128Mi"   # 1.5x headroom @ 300 RPS
```

**Note on Memory:** Memory usage scales slowly with RPS (24 MB @ 300 RPS, 83 MB @ 400 RPS). The 256Mi limit provides 3x headroom for the absolute maximum load, which is more than sufficient. This is an **87.5% reduction** from initial 2Gi estimates.

**Horizontal Scaling:**
```yaml
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70  # Scale at ~300 RPS per pod
```

**Expected Capacity:**
- Single pod: 300 RPS @ 70% CPU (recommended)
- Single pod: ~350 RPS @ 80% CPU (safe maximum)
- **DO NOT exceed 380 RPS** (CPU saturation, latency >1000ms)
- 3 pods (min): 900-1,050 RPS
- 10 pods (max): 3,000-3,500 RPS

**Improved Efficiency:**
- **87.5% memory reduction** (256Mi vs 2Gi initial estimate)
- Can run **8x more pods** on the same hardware
- CPU is the limiting factor, not memory

---

### Event Handler Production Configuration

**Container Resources:**
```yaml
challenge-event-handler:
  resources:
    limits:
      cpu: "0.5"         # 500m CPU for ~500 EPS
      memory: "256Mi"    # Generous headroom (actual: <100 MB)
    requests:
      cpu: "0.25"        # Baseline for 250+ EPS
      memory: "128Mi"    # 2x actual usage
```

**Configuration Parameters:**
```yaml
env:
  FLUSH_INTERVAL_MS: "100"         # 100ms flush interval
  BUFFER_SIZE: "3000"              # 3,000 records per partition
  NUM_FLUSH_WORKERS: "8"           # 8 parallel flush workers
  BACKPRESSURE_THRESHOLD: "4500"   # 150% of buffer size
  CIRCUIT_BREAKER_LEVEL: "7500"    # 250% of buffer size
```

**PostgreSQL Configuration:**
```yaml
challenge-postgres:
  resources:
    limits:
      cpu: "4"           # 4 cores for ~500 EPS capacity
      memory: "2Gi"      # Standard PostgreSQL allocation
    requests:
      cpu: "2"           # Baseline
      memory: "1Gi"
  config:
    shared_buffers: "512MB"
    max_connections: "150"
    work_mem: "16MB"
```

**Horizontal Scaling:**
```yaml
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 60  # Scale at ~300 EPS per pod
```

**Expected Capacity (Event Handler):**
- Single pod: 400 EPS @ 60% CPU (recommended)
- Single pod: ~500 EPS @ 75% CPU (safe maximum)
- 2 pods (min): 800-1,000 EPS
- 10 pods (max): 4,000-5,000 EPS

**Resource Efficiency:**
- **25% CPU usage @ 494 EPS** (4x headroom available)
- **<100 MB memory** (consistent across all load levels)
- **Zero backpressure** at target load
- **23% PostgreSQL CPU** (4 cores, excellent headroom)

---

### Monitoring

**Challenge Service Key Metrics:**

1. **Throughput:** requests/sec per pod (target: 250-300 RPS, max safe: 350 RPS)
2. **CPU Usage:** Should stay below 80% (autoscale at 70%)
3. **Latency:** p95 should stay below 6ms (normal @ 300 RPS: 3.63ms)
4. **Memory:** Should stay below 128 MiB (typical: 24-83 MiB)
5. **Error Rate:** Should be 0%

**Event Handler Key Metrics:**

1. **Throughput:** events/sec per pod (target: 300-400 EPS, max safe: 500 EPS)
2. **CPU Usage:** Should stay below 75% (autoscale at 60%)
3. **Latency:** p95 should stay below 50ms (normal @ 494 EPS: 21ms)
4. **Backpressure Activations:** Should be 0 (indicates buffer overflow)
5. **Flush Success Rate:** Should be 100%
6. **PostgreSQL CPU:** Should stay below 50% (with 4 cores)
7. **Per-Partition Balance:** Variance should be <5% (indicates good hash distribution)

**Challenge Service Alerts:**
```yaml
- alert: HighThroughput
  expr: requests_per_second > 350
  for: 2m
  description: RPS exceeding safe maximum, scale immediately

- alert: CriticalThroughput
  expr: requests_per_second > 380
  for: 30s
  severity: critical
  description: RPS at CPU saturation point, severe latency degradation expected

- alert: HighCPUUsage
  expr: cpu_usage > 80%
  for: 5m
  description: CPU usage high, approaching saturation (~350+ RPS)

- alert: CriticalCPUUsage
  expr: cpu_usage > 95%
  for: 1m
  severity: critical
  description: CPU near saturation, latency degradation likely

- alert: HighLatency
  expr: p95_latency > 10ms
  for: 2m
  description: Latency degraded (normal: 3-6ms), possible CPU saturation

- alert: CriticalLatency
  expr: p95_latency > 100ms
  for: 30s
  severity: critical
  description: Severe latency degradation, CPU saturated

- alert: HighMemory
  expr: memory_usage > 128Mi
  for: 5m
  description: Memory usage elevated (normal: 24-83 MiB)

- alert: VeryHighMemory
  expr: memory_usage > 192Mi
  for: 1m
  description: Memory usage critically high (75% of limit)
```

**Event Handler Alerts:**
```yaml
- alert: HighEventThroughput
  expr: events_per_second > 450
  for: 2m
  description: EPS approaching safe maximum (500), consider scaling

- alert: CriticalEventThroughput
  expr: events_per_second > 500
  for: 30s
  severity: critical
  description: EPS at capacity limit, may need additional pods

- alert: BackpressureActivated
  expr: backpressure_activations > 0
  for: 1m
  severity: warning
  description: Buffer overflow detected, system cannot keep up with event rate

- alert: CircuitBreakerTriggered
  expr: circuit_breaker_drops > 0
  for: 30s
  severity: critical
  description: Events being dropped due to buffer overflow, data loss occurring

- alert: HighFlushLatency
  expr: p95_flush_time > 100ms
  for: 5m
  description: Flush time degraded (normal: 40-50ms), PostgreSQL may be saturated

- alert: FlushFailures
  expr: flush_failure_rate > 0.01
  for: 2m
  severity: critical
  description: Database flush failures detected, investigate PostgreSQL health

- alert: PostgreSQLCPUSaturation
  expr: postgres_cpu_usage > 75%
  for: 5m
  description: PostgreSQL CPU high, consider increasing CPU allocation

- alert: PartitionImbalance
  expr: partition_variance > 10%
  for: 10m
  description: Hash distribution imbalanced, investigate partition logic

- alert: EventHandlerHighLatency
  expr: p95_event_latency > 100ms
  for: 2m
  description: Event processing latency degraded (normal: 21ms), investigate bottleneck
```

---

## Future Optimization Opportunities

### Challenge Service Optimizations

#### Priority 1: Optimize processGoalsArray (Optional)

**Current:** Character-by-character parsing (2.58s flat, 17.46%)

**Approach:** Use `bytes.Index` for bulk scanning instead of character loop

**Expected Gain:** ~10-15% additional CPU reduction

**Effort:** 2-3 hours

**Priority:** LOW - current performance already meets targets

#### Priority 2: Enable Enum Numbers (Breaking Change)

**Current:** Enums encoded as strings ("in_progress", "WALLET")

**Approach:** Enable `UseEnumNumbers: true` in protojson

**Impact:** 5% CPU reduction, smaller payloads

**Blockers:**
- Requires updating demo app
- Breaking change for API clients
- Consider API versioning (/v2)

**Effort:** 1 day (app updates + testing)

**Priority:** MEDIUM - wait for next API version

#### Priority 3: Response Streaming (HTTP/2)

**Current:** Buffered JSON response

**Approach:** Stream JSON array elements as they're built

**Impact:** 5% CPU, 10% memory reduction

**Complexity:** Medium (requires HTTP/2 support)

**Effort:** 4 hours

**Priority:** LOW - marginal gains

#### Priority 4: Cache User Progress (Redis)

**Current:** PostgreSQL query per request

**Approach:** Cache frequent users in Redis

**Impact:** 10-15% latency reduction

**Trade-offs:** Eventual consistency, cache invalidation complexity

**Effort:** 1-2 days

**Priority:** MEDIUM - consider for M3

---

### Event Handler Optimizations

#### Priority 1: Increase to 16 Flush Workers (Easy Win)

**Current:** 8 parallel flush workers

**Approach:** Increase to 16 workers with finer-grained partitioning

**Expected Gain:** 20-30% additional throughput (500 ‚Üí 650 EPS)

**Trade-offs:** More goroutines, slightly higher memory

**Effort:** 1 hour (configuration change)

**Priority:** HIGH - easy capacity increase if needed

#### Priority 2: PostgreSQL Connection Pooling Tuning

**Current:** Default connection pooling settings

**Approach:** Optimize pool size and idle connection management

**Expected Gain:** 10-15% flush time reduction

**Configuration:**
```go
db.SetMaxOpenConns(50)      // Default: 0 (unlimited)
db.SetMaxIdleConns(25)      // Default: 2
db.SetConnMaxLifetime(5m)   // Default: unlimited
```

**Effort:** 2 hours (testing + validation)

**Priority:** MEDIUM - marginal gains

#### Priority 3: Batch Size Auto-Tuning

**Current:** Fixed 3,000 record buffer per partition

**Approach:** Dynamic batch sizing based on event rate

**Expected Gain:** 5-10% latency improvement under varying load

**Logic:**
- Low load (<100 EPS): Flush at 100ms or 500 records
- Medium load (100-300 EPS): Flush at 100ms or 1,500 records
- High load (300+ EPS): Flush at 100ms or 3,000 records

**Effort:** 1 day (implementation + testing)

**Priority:** LOW - current fixed sizing works well

#### Priority 4: PostgreSQL Batch COPY Optimization

**Current:** Single COPY statement per partition

**Approach:** Batch multiple partition flushes into single transaction

**Expected Gain:** 15-20% flush time reduction

**Trade-offs:** More complex error handling, potential for cross-partition blocking

**Effort:** 2 days (implementation + testing)

**Priority:** LOW - risky, current COPY performance is excellent

#### Priority 5: Event Deduplication (User Protection)

**Current:** Map-based deduplication within buffer (per user-goal)

**Approach:** Add Redis-based deduplication for cross-instance protection

**Impact:** Prevent duplicate progress updates during pod restarts

**Use Case:** High-availability deployments with multiple pods

**Effort:** 2-3 days (Redis integration + testing)

**Priority:** MEDIUM - consider for production HA deployment

---

## Conclusion

### Summary of Achievements

**Challenge Service (REST API):**

‚úÖ **Phase 1 Complete:**
- Fixed critical memory bugs
- Eliminated 44% of allocations
- Achieved service stability

‚úÖ **Phase 2 Complete:**
- Eliminated 46% CPU bottleneck (protojson marshaling)
- Achieved **1.5-1.75x throughput improvement** (200 ‚Üí 300-350 RPS safe capacity)
- Maintained 100% correctness (0% errors)
- **Note:** System hits CPU saturation at 400 RPS (101% CPU, catastrophic latency)

**Event Handler (gRPC Events):**

‚úÖ **Phase 1 Complete:**
- Identified PostgreSQL bottleneck (100% CPU saturation)
- Measured 48% data loss at 500 EPS baseline

‚úÖ **Phase 2 Complete:**
- Implemented PostgreSQL COPY protocol (5-10x faster)
- Eliminated backpressure (47,990 ‚Üí 0 activations)
- Achieved 99.99% success rate

‚úÖ **Phase 3 Complete:**
- Optimized flush interval (1000ms ‚Üí 100ms)
- Increased buffer capacity (1,000 ‚Üí 3,000 records)

‚úÖ **Phase 3.5 Complete:**
- Doubled PostgreSQL CPU allocation (2 ‚Üí 4 cores)
- Achieved 95% of target throughput (475 EPS)

‚úÖ **Phase 4 Complete:**
- Implemented 8 parallel flush workers with hash-based partitioning
- Eliminated single-threaded flush bottleneck
- Reduced P95 latency from 10,000ms to 44ms (227x improvement)

‚úÖ **Phase 4b Complete:**
- Fixed k6 load generator VU limitations
- Achieved **2.07x throughput improvement** (239 ‚Üí 494 EPS)
- Final result: **98.7% of 500 EPS target**

### Final Status

**Challenge Service:**

| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| **Throughput** | 500 RPS | **~300-350 RPS (safe)** | ‚ö†Ô∏è 60-70% of target (CPU-limited) |
| **CPU Efficiency** | < 85% @ 500 RPS | **65-75% @ 300-350 RPS** | ‚úÖ Optimal at safe capacity |
| **Latency** | < 200ms p95 | **3.63ms p95 @ 300 RPS** | ‚úÖ 55x better |
| **Stability** | No crashes | **No crashes** | ‚úÖ Perfect |
| **Error Rate** | < 1% | **0%** | ‚úÖ Perfect |
| **CPU Saturation** | Avoid | **400 RPS: 101% CPU, 1,442ms P95** | ‚ùå Hard limit at ~380 RPS |

**Event Handler:**

| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| **Throughput** | 500 EPS | **494 EPS** | ‚úÖ 98.7% of target |
| **Success Rate** | > 99% | **100%** | ‚úÖ Perfect |
| **Latency** | < 500ms p95 | **21ms p95** | ‚úÖ 24x better |
| **Backpressure** | Minimal | **0 activations** | ‚úÖ Eliminated |
| **PostgreSQL CPU** | < 75% | **23%** | ‚úÖ Excellent headroom |

### Deployment Status

üöÄ **BOTH SERVICES READY FOR PRODUCTION**

All services have been optimized and thoroughly tested. Critical bottlenecks have been eliminated:

**Challenge Service:**
- **1.5-1.75x throughput improvement** (200 ‚Üí 300-350 RPS safe capacity)
- Memory usage reduced by 87.5% (256Mi vs 2Gi)
- ‚ö†Ô∏è **CPU-limited:** Hits saturation at 400 RPS (101% CPU, 397x latency degradation)
- Can scale horizontally to 2,400-2,800 RPS (8 pods @ 300-350 RPS each)

**Event Handler:**
- **2.07x throughput improvement** with zero data loss
- PostgreSQL COPY protocol + 8 parallel flush workers
- Hash-based partitioning with perfect load distribution (¬±1.8%)
- Can scale horizontally to 4,000-5,000 EPS (10 pods)

**Recommended Next Steps:**
1. ‚úÖ Deploy both services to production
2. üìä Monitor actual capacity under production load
3. üîÑ Validate horizontal scaling behavior
4. üìà Consider Event Handler Priority 1 optimization (16 workers) if >500 EPS needed
5. üéØ Benchmark production SSD-backed PostgreSQL (may unlock additional capacity)

---

## Appendix: File Changes Summary

### Challenge Service

**New Files Created:**

1. `extend-challenge-service/pkg/cache/serialized_challenge_cache.go` (252 lines)
2. `extend-challenge-service/pkg/response/json_injector.go` (365 lines)
3. `extend-challenge-service/pkg/response/builder.go` (189 lines)
4. `extend-challenge-service/pkg/response/json_injector_test.go` (comprehensive tests)
5. `extend-challenge-service/pkg/handler/optimized_challenges_handler.go`
6. `extend-challenge-service/pkg/common/buffer_pool.go` (buffer pooling utilities)

**Files Modified:**

1. `extend-challenge-service/pkg/common/sonic_marshaler.go` - Removed intermediate unmarshal
2. `extend-challenge-service/pkg/common/gateway.go` - Added gRPC buffer configuration
3. `extend-challenge-service/pkg/mapper/challenge_mapper.go` - Optimized time formatting
4. `extend-challenge-service/pkg/server/challenge_service_server.go` - Removed pool returns
5. `extend-challenge-service/main.go` - Register optimized handler and warmup cache

### Event Handler

**New Files Created:**

1. `extend-challenge-event-handler/internal/buffered/partitioned_buffered_repository.go` (hash-based partitioning, 8 workers)
2. `extend-challenge-event-handler/internal/repository/postgres_repository.go` (PostgreSQL COPY protocol implementation)

**Files Modified:**

1. `extend-challenge-event-handler/main.go` - Configure PartitionedBufferedRepository (8 workers, 100ms flush, 3000 buffer)
2. `extend-challenge-event-handler/internal/buffered/buffered_repository.go` - Updated backpressure thresholds (4500/7500)
3. `test/k6/scenario2_event_load.js` - Fixed connection reuse, increased VUs (1000/1500)
4. `docker-compose.yml` - Increased PostgreSQL CPUs (2 ‚Üí 4)

### Documentation

**Consolidated into this document:**

1. `docs/BACKPRESSURE_500EPS_RESULTS.md` ‚Üí Event Handler Phase 1
2. `docs/EVENT_HANDLER_500EPS_FAILURE_ANALYSIS.md` ‚Üí Event Handler Phase 1
3. `docs/EVENT_HANDLER_BUFFER_OVERFLOW_SOLUTION.md` ‚Üí Event Handler Phase 1
4. `docs/EVENT_HANDLER_PROFILE_ANALYSIS.md` ‚Üí Event Handler Phase 1
5. `docs/GOROUTINE_LEAK_FIX_RESULTS.md` ‚Üí Event Handler Phase 2
6. `docs/K6_FIX_COMPARISON.md` ‚Üí Event Handler Phase 2
7. `docs/K6_FIX_RESULTS_ANALYSIS.md` ‚Üí Event Handler Phase 2
8. `docs/LOGGING_LIBRARY_COMPARISON.md` ‚Üí Reference material
9. `docs/PERFORMANCE_CLIFF_VISUALIZATION.md` ‚Üí Challenge Service Phase 2
10. `docs/PHASE2_COPY_IMPLEMENTATION_SUMMARY.md` ‚Üí Event Handler Phase 2
11. `docs/PHASE2_COPY_LOAD_TEST_RESULTS.md` ‚Üí Event Handler Phase 2
12. `docs/PHASE3_4CPU_ANALYSIS.md` ‚Üí Event Handler Phase 3.5
13. `docs/PHASE3_500EPS_RESULTS.md` ‚Üí Event Handler Phase 3
14. `docs/PHASE3_OPTIMIZATION_SUMMARY.md` ‚Üí Event Handler Phase 3
15. `docs/PHASE4B_INCREASED_VUS_RESULTS.md` ‚Üí Event Handler Phase 4b
16. `docs/PHASE4_8_WORKERS_ANALYSIS.md` ‚Üí Event Handler Phase 4

**Final Document:**

- `docs/TECH_SPEC_M2_OPTIMIZATION.md` - This comprehensive document (Challenge Service + Event Handler)

---

**Document Version:** 3.0
**Last Updated:** 2025-10-29
**Status:** Complete ‚úÖ (Both Services Optimized)
